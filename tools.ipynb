{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Library"],"metadata":{"id":"0NdFyI-BRMbu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad9490d1"},"outputs":[],"source":["import pandas as pd\n","import re\n","import numpy as np\n","import nltk\n","import spacy\n","import random\n","import pickle\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from textblob import TextBlob\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tag import pos_tag\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n","\n","from joblib import dump, load\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","from wordcloud import WordCloud\n","\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Embedding, SpatialDropout1D, Bidirectional\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import load_model\n","\n","from gensim.models import Word2Vec\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","%load_ext autotime"]},{"cell_type":"code","source":["MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'Models')"],"metadata":{"id":"zf4Sl5RTHECJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tools"],"metadata":{"id":"rHZQOvXURH6o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"266da004"},"outputs":[],"source":["def _lowercase(text):\n","    \"\"\"\n","      Lowercase text\n","    \"\"\"\n","\n","    # Count the number of uppercase\n","    uppercase_count = sum(1 for s in text if s.isupper())\n","\n","    return text.lower(), uppercase_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80f348f8"},"outputs":[],"source":["def _remove_links(text):\n","    \"\"\"\n","      Remove web links\n","    \"\"\"\n","\n","    # Count the number of links\n","    links = re.findall(r'https?://\\S+', text)\n","\n","    # Remove all links\n","    clean_text = re.sub(r'https?://\\S+', '', text)\n","\n","    return clean_text, len(links)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5b61df50"},"outputs":[],"source":["def _remove_noise(text):\n","    \"\"\"\n","      Remove all special characters and number\n","    \"\"\"\n","\n","    # Count the number of digits in the text\n","    num_digits = len(re.findall(r'\\d', text))\n","\n","    # Count the number of special characters in the text\n","    num_special_chars = len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n","\n","    # Remove escape characters from the text\n","    clean_text = re.sub(r'\\\\[rntbf]', '', text)\n","\n","    # Remove all non-alphabetic characters from the text\n","    clean_text = re.sub(re.compile(r'[^a-zA-Z\\s]'), '', clean_text)\n","\n","    return clean_text, num_digits, num_special_chars"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57333bd6"},"outputs":[],"source":["def _wordtokenize(sent):\n","    \"\"\"\n","      Get word tokens from sentence\n","    \"\"\"\n","\n","    return word_tokenize(sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79c95f50"},"outputs":[],"source":["def _remove_stopwords(text):\n","    \"\"\"\n","      Filter out and count stopwords in each sentence\n","    \"\"\"\n","\n","    # Set of English stopwords from NLTK\n","    stop_words = set(stopwords.words('english'))\n","    new_sentence = []\n","    stopwords_count = 0\n","\n","    # Iterate through each token after splitting\n","    for token in text.split():\n","\n","        # Increment count if token is a stopword\n","        if token in stop_words:\n","            stopwords_count += 1\n","        else:\n","            # Combine non-stopword to sentence\n","            new_sentence.append(token)\n","\n","    return \" \".join(new_sentence), stopwords_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3bc4919"},"outputs":[],"source":["def _lemmatizer(text):\n","    \"\"\"\n","      Lemmatize tokens\n","    \"\"\"\n","\n","    # Initialize WordNet Lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","\n","    # Tokenize the input text\n","    tokens = word_tokenize(text)\n","\n","    # Lemmatize each token\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    # Join the lemmatized tokens into a sentence\n","    lemmatized_sentence = \" \".join(lemmatized_tokens)\n","\n","    return lemmatized_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7d3b49b"},"outputs":[],"source":["def _spell_check(text):\n","    \"\"\"\n","      Correct misspelled words\n","    \"\"\"\n","\n","    # Create a TextBlob object\n","    blob = TextBlob(text)\n","\n","    # Correct misspelled words\n","    corrected_text = blob.correct()\n","\n","    # Calculate the number of misspelled words\n","    num_misspelled = len(blob.words) - len(corrected_text.words)\n","\n","    return str(corrected_text), num_misspelled"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dae0d83"},"outputs":[],"source":["def _ner(text):\n","    \"\"\"\n","      Perform  Named Entity Recognition\n","    \"\"\"\n","\n","    # Load the English model\n","    ner = spacy.load(\"en_core_web_sm\")\n","\n","    # Process using NER model\n","    doc = ner(text)\n","\n","    # Extract named entities and their labels\n","    ner_features = {ent.text: ent.label_ for ent in doc.ents}\n","\n","    return ner_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"005a9a1d"},"outputs":[],"source":["def _pos_tagging(tokens):\n","    \"\"\"\n","      Perform Part of Speech tagging\n","    \"\"\"\n","\n","    # Perform POS tagging on tokens\n","    pos_tags = pos_tag(tokens)\n","\n","    # Store POS tags for each token\n","    pos_features = {word: pos for word, pos in pos_tags}\n","\n","    return pos_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"631f8fd8"},"outputs":[],"source":["def _import_data():\n","    \"\"\"\n","      Import dataset\n","    \"\"\"\n","\n","    # Read all source files\n","    fin_df = pd.read_json(path_or_buf=f'{GOOGLE_DRIVE_PATH}/source/finance.jsonl', lines=True)\n","    med_df = pd.read_json(path_or_buf=f'{GOOGLE_DRIVE_PATH}/source/medicine.jsonl', lines=True)\n","    openqa_df = pd.read_json(path_or_buf=f'{GOOGLE_DRIVE_PATH}/source/open_qa.jsonl', lines=True)\n","    reddit_df = pd.read_json(path_or_buf=f'{GOOGLE_DRIVE_PATH}/source/reddit_eli5.jsonl', lines=True)\n","    wiki_df = pd.read_json(path_or_buf=f'{GOOGLE_DRIVE_PATH}/source/wiki_csai.jsonl', lines=True)\n","\n","    # Assign source names\n","    fin_df['type'] = 'finance'\n","    med_df['type'] = 'medicine'\n","    openqa_df['type'] = 'open_qa'\n","    reddit_df['type'] = 'reddit_eli5'\n","    wiki_df['type'] = 'wiki_csai'\n","\n","    # Combine all datasets\n","    df = pd.concat([fin_df, med_df, openqa_df, reddit_df, wiki_df], ignore_index=True)\n","\n","    # Transform answer columns of human and AI to be in single column\n","    df_human = pd.DataFrame(df[['human_answers', 'type']].copy())\n","    df_human.columns = ['answer', 'type']\n","    df_human['ai-generated'] = 0\n","\n","    df_gpt = pd.DataFrame(df[['chatgpt_answers', 'type']].copy())\n","    df_gpt.columns = ['answer', 'type']\n","    df_gpt['ai-generated'] = 1\n","\n","    df = pd.concat([df_human, df_gpt])\n","\n","    # Shuffle records to reduce bias\n","    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    # Split multiple answers into each record\n","    df = df.explode('answer')\n","\n","    # Drop null answer\n","    df.dropna(subset=['answer'], inplace=True)\n","\n","    # Reset Index\n","    df.reset_index(drop=True, inplace=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fbc05e7"},"outputs":[],"source":["def _split_test_set(df, test_size=0.2, validation_size=0.2):\n","    \"\"\"\n","      Split to train-validation-test set\n","    \"\"\"\n","\n","    X = df['answer']\n","    y = df[['ai-generated', 'type']]\n","\n","    validation_pct = validation_size/(1-test_size)\n","\n","    # Split into train and test set\n","    X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                        y, test_size=test_size, random_state=42)\n","\n","\n","    # Split train set into valudation set\n","    X_train, X_val, y_train, y_val = train_test_split(X_train,\n","                                                      y_train, test_size=validation_pct, random_state=42)\n","\n","\n","    return X_train, X_val, y_train, y_val, X_test, y_test"]},{"cell_type":"code","source":["def count_words(text):\n","    \"\"\"\n","      Count number of words\n","    \"\"\"\n","\n","    return len(text.split())"],"metadata":{"id":"aFMO8p5hrtKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c09cadae"},"outputs":[],"source":["def _data_cleaning(df):\n","    \"\"\"\n","      Data Preparation, converting to lower case, removing all links, special characters, stopwords, and perform Lemmatization\n","    \"\"\"\n","\n","    # Convert to lowercase\n","    df['answer'], df['uppercase_count'] = zip(*df['answer'].apply(_lowercase))\n","\n","    # Remove all links\n","    df['answer'], df['link_count'] = zip(*df['answer'].apply(_remove_links))\n","\n","    # Count number of words\n","    df['tokens_count'] = df['answer'].apply(count_words)\n","\n","    # Remove all special characters\n","    df['answer'], df['num_digits'], df['num_special_chars'] = zip(*df['answer'].apply(_remove_noise))\n","\n","    # Remove stopwords\n","    # df['answer'], df['stopwords_count'] = zip(*df['answer'].apply(_remove_stopwords))\n","\n","    # Lemmatization\n","    df['answer'] = df['answer'].apply(_lemmatizer)\n","\n","    return df"]},{"cell_type":"code","source":["def _load_model(fn, model_type):\n","    \"\"\"\n","      Load Models\n","    \"\"\"\n","\n","    if model_type == 'Baseline':\n","        with open(f'{MODEL_PATH}/{fn}', 'rb') as f:\n","            clf = pickle.load(f)\n","\n","    if not model_type in ('BERT', 'LSTM'):\n","        clf = load(f'{MODEL_PATH}/{fn}')\n","\n","    else:\n","      clf = load_model(f'{MODEL_PATH}/{fn}')\n","\n","    return clf"],"metadata":{"id":"urQ5AX2bOsNt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _predict(clf, X, y, model_type):\n","    \"\"\"\n","      Get predicted labels\n","    \"\"\"\n","\n","    if model_type == 'Baseline':\n","        y_pred = clf().predict(X, y)\n","\n","    elif not model_type in ('BERT', 'LSTM'):\n","        y_pred = clf.predict(X)\n","\n","    elif model_type == 'BERT':\n","        y_pred_prob = clf.predict(X)\n","\n","        # Get the predicted labels based on the highest probability\n","        y_pred = y_pred_prob.argmax(axis=1)\n","\n","    elif model_type == 'LSTM':\n","        y_pred_prob = clf.predict(X)\n","\n","        # Convert probabilities to binary labels using a threshold of 0.5\n","        y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","    return y_pred"],"metadata":{"id":"JGe7dl9TPOUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _evaluate_model(fn, X, y, model_type):\n","    \"\"\"\n","      Load, predict, and evaluate models' performance\n","    \"\"\"\n","\n","    # Load saved models\n","    clf = _load_model(fn, model_type)\n","\n","    # Get predicted labels, and generate performance matrix\n","    y_pred = _predict(clf, X, y, model_type)\n","    _performance_matrix(y['ai-generated'], y_pred)"],"metadata":{"id":"QW7CWtbxPcw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _performance_matrix(y, y_pred, matrix=True):\n","    \"\"\"\n","      Generate confusion matrix and classification report\n","    \"\"\"\n","\n","    # Visualize classification report\n","    print(classification_report(y, y_pred))\n","\n","    # Calculate and generate confusion matrix\n","    if matrix:\n","        cm = confusion_matrix(y, y_pred)\n","\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=['Human', 'AI-Generated'], yticklabels=['Human', 'AI-Generated'])\n","        plt.xlabel('Predicted')\n","        plt.ylabel('Actual')\n","        plt.title('Confusion Matrix')\n","        plt.show()"],"metadata":{"id":"__dmtHPotCAM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _history_plot(history, metric_name=['accuracy','val_accuracy', 'loss','val_loss']):\n","    \"\"\"\n","      Plot leanring history (accuracy and loss) for training and validation set through epochs\n","    \"\"\"\n","\n","    # Create subplots\n","    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n","\n","    # Convert history log to dataframe\n","    history_log_df = pd.DataFrame(history)\n","\n","    # Plot accuracy\n","    sns.lineplot(data=history_log_df[[metric_name[0], metric_name[1]]], ax=axs[0])\n","    axs[0].set_title('Accuracy')\n","\n","    # Plot loss\n","    sns.lineplot(data=history_log_df[[metric_name[2], metric_name[3]]], ax=axs[1])\n","    axs[1].set_title('Loss')"],"metadata":{"id":"xlJyKykUuxmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _evaluate_each_type(y_test, y_pred_test):\n","    \"\"\"\n","      Generate performance matrix for each subset of data from different sources\n","    \"\"\"\n","\n","    # Get unique source\n","    unique_types = y_test['type'].unique()\n","\n","    # Iterate to each source\n","    for type_value in unique_types:\n","        print('--------------------------------------------------')\n","        print(f\"Evaluating {type_value}\")\n","        print('--------------------------------------------------')\n","\n","        # Filter only currect source data\n","        y_test_fil = y_test[y_test['type'] == type_value]['ai-generated']\n","        y_pred_test = pd.DataFrame(y_pred_test)\n","        y_pred_test.index = y_test.index\n","        y_pred_fil = y_pred_test[y_test['type'] == type_value]\n","\n","        # Evaluate performance\n","        _performance_matrix(y_test_fil, y_pred_fil)"],"metadata":{"id":"rHU8aPbgRztC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _get_wrong_prediction(y_pred, source=None):\n","    \"\"\"\n","      Print out misprediction examples\n","    \"\"\"\n","\n","    # Get indices of misprediction\n","    wrong_idx = [i for i, (true_label, pred_label) in enumerate(zip(y_test['ai-generated'], y_pred)) if true_label != pred_label]\n","\n","    # Get 50 examples of misprediction\n","    n = 0\n","    for i, (src, true_label, pred_label) in enumerate(zip(y_test['type'], y_test['ai-generated'], y_pred)):\n","        if i in wrong_idx:\n","\n","              # Get examples from specific source\n","              if source is not None:\n","                  if src == source:\n","                      print(\"Raw Text:\", X_test_raw.iloc[i])\n","                      print(\"Processed Text:\", X_test.iloc[i])\n","                      print(\"True Label:\", true_label)\n","                      print(\"Predicted Label:\", pred_label)\n","                      n+=1\n","\n","              # Get examples from all sources\n","              else:\n","                print(\"Raw Text:\", X_test_raw.iloc[i])\n","                print(\"Processed Text:\", X_test.iloc[i])\n","                print(\"True Label:\", true_label)\n","                print(\"Predicted Label:\", pred_label)\n","                n+=1\n","\n","              if n == 50:\n","                break"],"metadata":{"id":"rBma8efIR0Yn"},"execution_count":null,"outputs":[]}]}